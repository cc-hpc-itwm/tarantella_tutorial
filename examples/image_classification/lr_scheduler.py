import tensorflow as tf
import numpy as np

BASE_LEARNING_RATE = 0.1  # This matches Jing's version.
LR_SCHEDULE = [    # (multiplier, epoch to start) tuples
    (1.0, 5), (0.1, 30), (0.01, 60), (0.001, 80)
]
MIN_BATCH_SIZE = 64 # min micro-batch size used for one rank

class LearningRateBatchScheduler(tf.keras.callbacks.Callback):
  """Callback to update learning rate on every batch (not epoch boundaries).

  N.B. Only support Keras optimizers, not TF optimizers.

  Attributes:
      schedule: a function that takes an epoch index and a batch index as input
          (both integer, indexed from 0) and returns a new learning rate as
          output (float).
  """

  def __init__(self, schedule, batch_size, num_images):
    super(LearningRateBatchScheduler, self).__init__()
    self.schedule = schedule
    self.batches_per_epoch = num_images // batch_size # ignore the remainder
    self.batch_size = batch_size
    self.epochs = -1
    self.prev_lr = -1

  def on_epoch_begin(self, epoch, logs=None):
    if not hasattr(self.model.optimizer, 'learning_rate'):
      raise ValueError('Optimizer must have a "learning_rate" attribute.')
    self.epochs += 1

  def on_batch_begin(self, batch, logs=None):
    """Executes before step begins."""
    lr = self.schedule(self.epochs,
                       batch,
                       self.batches_per_epoch,
                       self.batch_size)
    if not isinstance(lr, (float, np.float32, np.float64)):
      raise ValueError('The output of the "schedule" function should be float.')
    if lr != self.prev_lr:
      self.model.optimizer.learning_rate = lr  # lr should be a float here
      self.prev_lr = lr



def learning_rate_schedule(current_epoch,
                           current_batch,
                           batches_per_epoch,
                           batch_size):
  """Handles linear scaling rule, gradual warmup, and LR decay.

  Scale learning rate at epoch boundaries provided in LR_SCHEDULE by the
  provided scaling factor.

  Args:
    current_epoch: integer, current epoch indexed from 0.
    current_batch: integer, current batch in the current epoch, indexed from 0.
    batches_per_epoch: integer, number of steps in an epoch.
    batch_size: integer, total batch sized.

  Returns:
    Adjusted learning rate.
  """
  initial_lr = BASE_LEARNING_RATE * batch_size / MIN_BATCH_SIZE
  epoch = current_epoch + float(current_batch) / int(batches_per_epoch)
  warmup_lr_multiplier, warmup_end_epoch = LR_SCHEDULE[0]
  if epoch < warmup_end_epoch:
    # Learning rate increases linearly per step.
    return initial_lr * warmup_lr_multiplier * epoch / warmup_end_epoch
  for mult, start_epoch in LR_SCHEDULE:
    if epoch >= start_epoch:
      learning_rate = initial_lr * mult
    else:
      break
  return learning_rate


class ExpDecayWithWarmupSchedule(tf.keras.optimizers.schedules.ExponentialDecay):
  def __init__(self, base_learning_rate, num_ranks,
               decay_steps, decay_rate, staircase,
               warmup_steps, warmup_rate):
    # scale learning rate by the number of ranks
    initial_learning_rate = base_learning_rate * num_ranks

    self.warmup_rate = warmup_rate
    self.warmup_steps = warmup_steps
    super().__init__(initial_learning_rate, decay_steps, decay_rate, staircase)

  def __call__(self, step):
    def warmup():
      # Learning rate increases linearly per step.
      multiplier = self.warmup_rate * (step / self.warmup_steps)
      return tf.multiply(self.initial_learning_rate, multiplier)

    def exp_decay():
      return super(ExpDecayWithWarmupSchedule, self).__call__(step)

    return tf.cond(tf.math.less_equal(step, self.warmup_steps),
                    true_fn = warmup,
                    false_fn = exp_decay)



